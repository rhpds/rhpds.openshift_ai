---
ocp4_workload_openshift_ai_model_as_a_service_helm_repo: https://github.com/treddy08/model-as-a-service.git
ocp4_workload_openshift_ai_model_as_a_service_helm_repo_target_revision: main
ocp4_workload_openshift_ai_model_as_a_service_maas_user_count: 1
ocp4_workload_openshift_ai_model_as_a_service_api_users:
- count: 5
  username_prefix: api-user-
  tier: tier-free-users
ocp4_workload_openshift_ai_model_as_a_service_models:
- apiVersion: serving.kserve.io/v1alpha1
  kind: LLMInferenceService
  metadata:
    name: facebook-opt-125m
    namespace: llm
    annotations:
      alpha.maas.opendatahub.io/tiers: '[]'
  spec:
    model:
      uri: hf://facebook/opt-125m
      name: facebook-opt-125m
    replicas: 1
    router:
      route: { }
      gateway:
        refs:
          - name: maas-default-gateway
            namespace: openshift-ingress
    template:
      containers:
        - name: main
          image: quay.io/pierdipi/vllm-cpu:latest
          env:
            - name: VLLM_LOGGING_LEVEL
              value: INFO
          resources:
            limits:
              cpu: '2'
              memory: 10Gi
            requests:
              cpu: '100m'
              memory: 8Gi
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
              scheme: HTTPS
            initialDelaySeconds: 90
            periodSeconds: 30
            timeoutSeconds: 30
            failureThreshold: 5
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
              scheme: HTTPS
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 30
- apiVersion: serving.kserve.io/v1alpha1
  kind: LLMInferenceService
  metadata:
    name: granite-3-2-8b-instruct
    namespace: llm
    annotations:
      openshift.io/display-name: granite-3-2-8b-instruct
      alpha.maas.opendatahub.io/tiers: '[]'
      argocd.argoproj.io/sync-wave: "2"
    labels:
      opendatahub.io/dashboard: 'true'
  spec:
    model:
      name: granite-3-2-8b-instruct
      uri: 'oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.2-8b-instruct'
    replicas: 1
    router:
      route: { }
      # Connect to MaaS-enabled gateway
      gateway:
        refs:
          - name: maas-default-gateway
            namespace: openshift-ingress
    template:
      # Ensure the pod is scheduled on GPU nodes
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Exists
      containers:
      - name: main
        resources:
          limits:
            cpu: '2'
            memory: 12Gi
            nvidia.com/gpu: '1'
          requests:
            cpu: '500m'
            memory: 8Gi
            nvidia.com/gpu: '1'